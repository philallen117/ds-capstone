---
title: "Corpus data exploration"
author: "Phil Allen"
date: "1 May 2016"
output: html_document
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
# publish to http://rpubs.com/philallen117/ds-capstone-explore
knitr::opts_chunk$set(echo=FALSE)
library(parallel)
options(mc.cores = 4)
library(wordcloud)
library(sm)
library(quanteda)
```
```{r pp, cache=TRUE, cache.lazy=FALSE}
load("data/pp.8.RData")
```
```{r grams, cache=TRUE, dependson='pp'}
lenChars <- lapply(v, function(v){sort(nchar(v), decreasing=TRUE)})
lenWords <- lapply(dfm1, function(d){sort(rowSums(d), decreasing=TRUE)})
wordFreq <- lapply(dfm1, function(d){sort(colSums(d), decreasing=TRUE)})
gram2Freq <- lapply(dfm2, function(d){sort(colSums(d), decreasing=TRUE)})
gram3Freq <- lapply(dfm3, function(d){sort(colSums(d), decreasing=TRUE)})
```

# Objectives

The results are for the en-US corpora, only. 

* Discover appropriate pre-processing.
* Explore frequencies of patterns of words and n-grams.
* Compare these between corpora for blogs, tweets, and news items.

# Data source

Contributed by swiftkey over an unknown time period ? and unknown number of users. All we are given is the documents themselves. However, we have corpore from differnt online media, whose variety we can use to test the robustness of our approaches.

# Processing

The blog data reads successfully as UTF-8. Since the target application is simple text on mobile devices, I decided it to transliterate to ASCII. I also removed punctuation including quoting, and digits, since none of these are high priorities targets for text prediction. (There could be valuable special cases, such as "2016", but I will deal with those later.)

??? Ellided words such as "don't" are preserved. Capitalisation is also preserved, with the aim of later recognising proper nouns such as "Julia" or "Brazil".

I applied the same pre-processing to all corpora for purposes of comparison. That probably won't work for tweets because of tags and IDs. Tags could be benefit from the use of a dictionary to separate word boundaries, e.g #tellitlikeitis. IDs are less regular (in the quest to avoid clashes). They really need a combination of directory support and remembering past choices by the user.

# Gross structural features of the corpora

Table for words and characters.

Tweets are necessarily shorter than the media without a technical limit. We also see shorter words in tweets in comparison with the other media.

# Feature frequency

## Distribution of single words

What should a Zipf distribution look like?

Even taking logs, ...

Differences?

## Most frequent single words

Tables

```{r top.word, echo=FALSE, results='asis'}
top.n.word <- as.data.frame(lapply(lapply(wordFreq, head, n=12), names))
knitr::kable(top.n.word)
```

Look at word clouds.

```{r wordclouds, echo=FALSE, fig.keep='all', dependson='grams'}
old.par <- par(mfrow = c(1, 3))
wc <- function(wf, t) { 
  wordcloud(names(wf), wf, max.words=50) # 100 takes a while
  title(paste("Frequent in", t))
}
wc(wordFreq[["twitter"]], "twitter")
wc(wordFreq[["blogs"]], "blogs")
wc(wordFreq[["news"]], "news")
par(old.par)
```

As noted above, I have not removed stop words. So we see a great deal of common articles, preposions, connectives. In comparison with news, blogs introduce first person pronouns. Tweets also use second person pronouns; though are broadcast, they are often addressed to one person.

## Distribution of bigrams

## Most frequent bigrams

```{r top.gram2, echo=FALSE, results='asis'}
top.gram2 <- as.data.frame(lapply(lapply(gram2Freq, head, n=12), names))
knitr::kable(top.gram2)
```


## Distribution of trigrams

## Most frequent trigrams

```{r top.gram3, echo=FALSE, results='asis'}
top.gram3 <- as.data.frame(lapply(lapply(gram3Freq, head, n=12), names))
knitr::kable(top.gram3)
```


Short sequences denoting common relationships between words or phrases, such as "on top of".

Again, tweets are personally addressed: people say thank you, and "I love you". They also refer to plans, e.g. "I want to", "going to be".

News brings in more of those common relationships ("as well as", "one of the"), and we reported speech ("said in a").

Blogs have something in common with news, but again the first person phrases come in, as well as propositional attitudes (ref!!!)

# Predictive value of n-grams

## Approach

How far do we have to go in the predictive model. E.g. from bigrams we could build a unigram Markov model, from trigrams we could build a bigram Markov model.

Could compare freqency distribution of bigrams with same first term, trigrams with same first two terms. Raw totals of possible suffixes might be misleading, because some of the suffixes may be very rare. It is better to pick some quantile, e.g. the 90% most common continuations. So we would compare the distribution of these 90% quantiles of the numbers of suffixes, for bigrams and trigrams.

Are there methods on Markov chain to prune a tail quantile, and so prune the number of arcs in the Markov chain? The distribution of the out-degree of the nodes in the Markov chain would be what we are trying to compare.

We expect these distributions also to be skew, so we would not compare means, but rather median and tail-ish percentile, e.g. 90%.

## Results




??? Are we doing whole words or rest of current word?




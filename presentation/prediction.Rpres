Text prediction data product
========================================================
author: Phil Allen
date: 6 June 2016
autosize: true

Summary
========================================================

#### Introduction

This presentation describes a product for predicting the next word that a user will type. It describes the data used, the model and its training, and the application of the model in prediction. Finally, it describes how to use the product.

#### Product objective

The objective of the product is to help the user to type more quickly, by suggesting likely next words. The intended application is for touch devices.

#### Links

- [Source code](https://github.com/philallen117/ds-capstone)
- [Data Product](https://philallen117.shinyapps.io/ds-capstone/)


Training Data and Preparation
============================================================

The data comprises corpora of tweets, news items, and and blog posts provided by SwiftKey. Only the text in English corpora have been used for this product. Different languages would require different models.

The documents were sampled in ratio 3:1 for training and test data. The preparation applied to the data was:

* Removal of profanities, white space, punctuation, and twitter tags using [tm](https://cran.r-project.org/web/packages/tm/index.html).
* Transformation to lower case.

Model Training
========================================================

The product uses a Markov model for the next word, given a preceeding n-gram, where $0 \le n \le 4$.  The model was trained from around 10% of the pre-processed data.

*  The corpora were parsed into sentences using [OpenNLP](https://cran.r-project.org/web/packages/openNLP/index.html). This avoids counting n-grams that span sentences, e.g. "so quickly. Reluctantly, I opened".

* Frequencies of unigrams up to 5-grams were calculated. Thence, relative frequencies of the next word from 0 to 4-grams were calculated.

* Unigram probabilities were adjusted for words that occur mostly in the context of a given bigram, e.g. "(San) Francisco". See _novel continuations_ in [Jurafsky & Martin](https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf).

* For each initial context (0-gram to 4-gram), the possible next words were ranked and a maximum of 6 retained.

  - A better approach to the cut-off could be cumulative probability, e.g. discard further words once 90%ile is reached.
  
Applying the Model for Prediction
========================================================

When the user types a phrase, the product first pre-processes the user's sentence as above. The product looks for matches using a simple back-off technique, as follows.

Set k = min(4, l) words, where l is the length in words of the pre-processed phrase.

Repeat until match found:

1. Look for a match in the model on the last k words in the user's phrase.

2. If there is a match, pick the continuation with the highest relative frequency and break.

3. If there is no match, set k = k - 1 and loop.


Instructions for using the Shiny app
========================================================

* Enter a phrase.
* Click the "predict" button to see suggestions.
* The predicted word will appear in bold font underneath. If the model has other possible continuations, these will appear in regular font underneath.

![UI](appScreenShot.png)
